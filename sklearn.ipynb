{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning Using Scikit-learn\n",
    "#### By Niko Gupta\n",
    "\n",
    "This tutorial will take you through the process, from start to finish, of using machine learning in a real application. You will learn how to use the scikit-learn (sklearn) library to train a model on a test set of data, and then use it to make predictions on new data. In addition, you will learn how to verify the correctness and quality of your machine learning model.\n",
    "\n",
    "This tutorial also covers an example of how to clean and prepare data for the model, along with how to use the Pickle library to save Python objects to files for later use. The example will show how to save the model, load it up again, and then make predictions on real data, much like you might do in a real application.\n",
    "\n",
    "The model we are going to train will take the text from a yelp review for a restaurant, and predict the rating for that review.\n",
    "\n",
    "#### Things the tutorial will cover:\n",
    "* [1) Data Collection](#Part-1:-Data-Collection)\n",
    "* [2) Cleaning the Data](#Part-2:-Cleaning-the-Data)\n",
    "* [3) Training the Model](#Part-3:-Training-the-Model)\n",
    "* [4) Evaluating the Model](#Part-4:-Evaluating-the-Model)\n",
    "* [5) Improving the Model](#Part-5:-Improving-the-Model)\n",
    "* [6) Further Reading](#Part-6:-Further-Reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up library imports\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from testing.testing import test\n",
    "\n",
    "# Global variables\n",
    "review_input_file = 'data/review.json'\n",
    "training_data_file = 'training_data.pickle'\n",
    "test_data_file = 'test_data.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "The first task in any machine learning application is collecting data. Typically, you will want 2 sets of data: a training set and a test set. The former will be used to train your model, and the latter will be used to test its effectiveness. While you can use 2 entirely different data sets, it is usually sufficient to just break one set into 2 parts.\n",
    "\n",
    "For this tutorial we will be using the [yelp dataset](https://www.yelp.com/dataset/). This dataset was released for students to use in data science applications. It is publicly accessible, but due to Yelp's limitations on on on caching their data, I cannot include the actual dataset in this tutorial. You can download the dataset [here](https://www.yelp.com/dataset/download) to follow along with the tutorial. After downloading the data, untar it and move the review.json file to a location of your choice. If the location is anything other than `./data/review.json`, change the global variable in the previous cell to reflect the location.\n",
    "\n",
    "The dataset includes interesting information such as business data, reviews, user information (including a pseudo social network through friend mappings), business checkins, restaurant reviews, and photo information. For this tutorial we are interested specifically in the restaurant reviews.\n",
    "\n",
    "One problem we encounter with the yelp dataset is that the uncompressed archive is 8 GB. In particular, the reviews.json file that we are interested in is 5 GB, containing over 6 million revies. If we were to load this entire file as is into RAM, it would cause an OS error. In addition, we don't need all 6 million reviews for training or testing the data. Instead we are going to hardcode the number of reviews we would like per dataset, and only read that many from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'review_id': 'Q1sbwvVQXV2734tPgoKj4Q', 'user_id': 'hG7b0MtEbXx5QzbzE6C_VA', 'business_id': 'ujmEBvifdJM6h6RLv4wQIg', 'stars': 1.0, 'useful': 6, 'funny': 1, 'cool': 0, 'text': 'Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.', 'date': '2013-05-07 04:34:36'} \n",
      "\n",
      "{'review_id': 'GJXCdrto3ASJOqKeVWPi6Q', 'user_id': 'yXQM5uF2jS6es16SJzNHfg', 'business_id': 'NZnhc2sEQy3RmzKTZnqtwQ', 'stars': 5.0, 'useful': 0, 'funny': 0, 'cool': 0, 'text': \"I *adore* Travis at the Hard Rock's new Kelly Cardenas Salon!  I'm always a fan of a great blowout and no stranger to the chains that offer this service; however, Travis has taken the flawless blowout to a whole new level!  \\n\\nTravis's greets you with his perfectly green swoosh in his otherwise perfectly styled black hair and a Vegas-worthy rockstar outfit.  Next comes the most relaxing and incredible shampoo -- where you get a full head message that could cure even the very worst migraine in minutes --- and the scented shampoo room.  Travis has freakishly strong fingers (in a good way) and use the perfect amount of pressure.  That was superb!  Then starts the glorious blowout... where not one, not two, but THREE people were involved in doing the best round-brush action my hair has ever seen.  The team of stylists clearly gets along extremely well, as it's evident from the way they talk to and help one another that it's really genuine and not some corporate requirement.  It was so much fun to be there! \\n\\nNext Travis started with the flat iron.  The way he flipped his wrist to get volume all around without over-doing it and making me look like a Texas pagent girl was admirable.  It's also worth noting that he didn't fry my hair -- something that I've had happen before with less skilled stylists.  At the end of the blowout & style my hair was perfectly bouncey and looked terrific.  The only thing better?  That this awesome blowout lasted for days! \\n\\nTravis, I will see you every single time I'm out in Vegas.  You make me feel beauuuutiful!\", 'date': '2017-01-14 21:30:33'} \n",
      "\n",
      "{'review_id': '2TzJjDVDEuAW6MR5Vuc1ug', 'user_id': 'n6-Gk65cPZL6Uz8qRm3NYw', 'business_id': 'WTqjgwHlXbSFevF32_DJVw', 'stars': 5.0, 'useful': 3, 'funny': 0, 'cool': 0, 'text': \"I have to say that this office really has it together, they are so organized and friendly!  Dr. J. Phillipp is a great dentist, very friendly and professional.  The dental assistants that helped in my procedure were amazing, Jewel and Bailey helped me to feel comfortable!  I don't have dental insurance, but they have this insurance through their office you can purchase for $80 something a year and this gave me 25% off all of my dental work, plus they helped me get signed up for care credit which I knew nothing about before this visit!  I highly recommend this office for the nice synergy the whole office has!\", 'date': '2016-11-09 20:09:03'} \n",
      "\n",
      "### TESTING get_json: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_size = 20000\n",
    "\n",
    "def get_json_test(get_json):\n",
    "    result = get_json(review_input_file)\n",
    "\n",
    "    # Look at first few reviews\n",
    "    print(result[0], '\\n')\n",
    "    print(result[1], '\\n')\n",
    "    print(result[2], '\\n')\n",
    "\n",
    "# Load the first dataset_size reviews from filename\n",
    "@test\n",
    "def get_json(filename):\n",
    "    seen = 0\n",
    "    result = []\n",
    "    with open(filename, 'rb') as json_file:\n",
    "        while (seen < dataset_size):\n",
    "            seen += 1\n",
    "            line = json_file.readline()\n",
    "            result.append(json.loads(line))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning the Data\n",
    "\n",
    "Now that we have the reviews file loaded into a dictionary, we need to reformat it into something that is more readily useable by a machine learning model. The operations we are interested in doing to the data in order to clean it are:\n",
    "* Remove data we don't care about, i.e. removing extra json fields\n",
    "* Standardize the reviews by removing non alphabetical characters, such as punctuation\n",
    "* Make all words lowercase so that the model doesn't treat the same words with different case as different features\n",
    "* Remove the most common words in the English language (such as 'the' and 'as') so that the model trains on words that are actually relevant within each review. Certain words that are listed in the top 100 words but seem like they might be relevant to the review would not make sense to remove. In this case, we don't remove `not`, `but`, `out`, `like`, `no`, `into`, `good`, `over`, `well`, and `most`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data:\n",
      " {'stars': 1.0, 'text': 'total bill horrible service over gs crooks actually had nerve charge pills checked online pills had cents each avoid hospital ers all costs'} \n",
      "\n",
      "### TESTING get_datasets: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_datasets_test(get_datasets):\n",
    "    training_data, _ = get_datasets()\n",
    "    print('Cleaned data:\\n', training_data[0], '\\n')\n",
    "\n",
    "# Given a single review, remove unnecessary columns\n",
    "def remove_extra(review):\n",
    "    review.pop('review_id')\n",
    "    review.pop('user_id')\n",
    "    review.pop('business_id')\n",
    "    review.pop('date')\n",
    "    review.pop('useful')\n",
    "    review.pop('funny')\n",
    "    review.pop('cool')\n",
    "    \n",
    "\n",
    "# Given a single review, clean its description\n",
    "def clean_description(review):\n",
    "    lower = review['text'].lower()\n",
    "    \n",
    "    # Remove all characters except a-z and ' '\n",
    "    all_ascii = filter(lambda i: 97 <= ord(i) <= 122 or ord(i) == 32, lower)\n",
    "    \n",
    "    # Convert the filter object into a list so we can remove common words\n",
    "    word_list = ''.join(all_ascii).split()\n",
    "    common_words = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'his', 'by', 'from', 'them', 'we', 'say', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'or', 'would', 'there', 'their', 'what',\n",
    "        'so', 'up', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
    "        'when', 'make', 'can', 'time', 'just', 'him', 'know', 'take',\n",
    "        'people', 'year', 'your', 'some', 'could', 'them', 'see', 'other',\n",
    "        'than', 'then', 'now', 'look', 'only', 'come', 'its', 'think', 'also',\n",
    "        'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first', 'way',\n",
    "        'even', 'new', 'want', 'because', 'any', 'these', 'give', 'day', 'us'\n",
    "        }\n",
    "    result = [word for word in word_list if word not in common_words]\n",
    "    \n",
    "    # Combine the result back into a string, and overwrite the original description\n",
    "    review['text'] = ' '.join(result)\n",
    "\n",
    "# Given a list of reviews, clean the descriptions and remove extra columns.\n",
    "#    Note: this function modifies the input in place\n",
    "def clean_reviews(reviews):\n",
    "    list(map(remove_extra, reviews))\n",
    "    list(map(clean_description, reviews))\n",
    "\n",
    "# Create the datasets we will use\n",
    "@test\n",
    "def get_datasets():\n",
    "    # Load and clean the data\n",
    "    data = get_json(review_input_file)\n",
    "    clean_reviews(data)\n",
    "\n",
    "    # Break it into the test and training sets\n",
    "    mid = dataset_size // 2\n",
    "    training_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "\n",
    "    # Save as pickle objects so we don't have to reload and clean the data every time\n",
    "    with open(training_data_file, 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "    \n",
    "    with open(test_data_file, 'wb') as file:\n",
    "        pickle.dump(test_data, file)\n",
    "    \n",
    "    return (training_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Training the Model\n",
    "\n",
    "The first part of training a machine learning model is identifying what kind of problem you have. It can fall into a few different categories:\n",
    "\n",
    "* __Unsupervised learning:__ the model is given a set of inputs, with no target outputs. The model will attempt to find correlations between the inputs, and use that to find correlation with future inputs.\n",
    "\n",
    "* __Supervised learning:__ the model is given both a set of inputs and the target outputs corresponding to those inputs. Depending on the type of the target output, this can be further broken down into classification and regression.\n",
    "\n",
    "    * __Classification:__ output is one of a set of categories. The goal is to find patterns that map the input to the correct category, and then for new input predict which output label to classify it with.\n",
    "\n",
    "    * __Regression:__ output is continuous. For example, if we were assigning expected GPA for students based on study time, absences, and age, the output would be continuous.\n",
    "\n",
    "In our case, the problem is a supervised, since we already know the ratings for each review in the training set.\n",
    "\n",
    "Various algorithms exist for training a model. We will start with a [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes) classifier, which will provide us with a good baseline.\n",
    "\n",
    "Before we move forward, a problem comes to light: machine learning models are effectively tuning a complicated equation using the training data, and plugging new inputs into that equation to \"predict\" the output. However, we have text data — which isn't easily useable as input for an equation. This means we need to do some further processing on the input datasets before we an use them.\n",
    "\n",
    "A common transformation used in natural language processing is the \"bag of words\" model. A string of text is parsed into a dictionary mapping each word to the frequency it appears within the text. Then each word is assigned a unique ID, and a separate data structure is kept to maintain this \"vocabulary mapping\". This new mapping of unique ID to frequency is turned into a 2 dimensional matrix, which can be used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model.pickle\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasty:  30337\n",
      "good:  12874\n",
      "bad:  2150 \n",
      "\n",
      "### TESTING train_model: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model_test(train_model):\n",
    "    train_model(MultinomialNB())\n",
    "\n",
    "@test\n",
    "def train_model(algorithm):\n",
    "    # Load our training data from the pickle file\n",
    "    with open(training_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # For training, we must separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # Generate our \"bag of words\" frequency mapping\n",
    "    count_vect = CountVectorizer()\n",
    "    bag_of_words = count_vect.fit_transform(text)\n",
    "    \n",
    "    # Visualize the data\n",
    "    print(\"tasty: \", count_vect.vocabulary_.get(u'tasty'))\n",
    "    print(\"good: \", count_vect.vocabulary_.get(u'good'))\n",
    "    print(\"bad: \", count_vect.vocabulary_.get(u'bad'), '\\n')\n",
    "\n",
    "    # Convert the bag of words to a 2d matrix\n",
    "    transformer = TfidfTransformer()\n",
    "    train_data = transformer.fit_transform(bag_of_words)\n",
    "    \n",
    "    # Training time!\n",
    "    with open(model_file, 'wb') as file:\n",
    "        model = algorithm.fit(train_data, stars)\n",
    "        pickle.dump((model, count_vect, transformer), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to streamline the process of vectorizing and transforming input text, then predicting the output for that input, sklearn provides the `Pipeline` class. This allows us to cleanly package everything together, and the above function simplifies to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING train_model_pipe: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_model_pipe_test(train_model_pipe):\n",
    "    train_model_pipe(MultinomialNB())\n",
    "\n",
    "@test\n",
    "def train_model_pipe(algorithm):\n",
    "    # Load our training data from the pickle file\n",
    "    with open(training_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # For training, we must separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # Note that the names here are arbitrary; they allow us\n",
    "    # to refer back to it later\n",
    "    model = Pipeline([\n",
    "        ('count_vect', CountVectorizer()),\n",
    "        ('transformer', TfidfTransformer()),\n",
    "        ('algorithm', algorithm)\n",
    "    ])\n",
    "    \n",
    "    # Training time!\n",
    "    with open(model_file, 'wb') as file:\n",
    "        model.fit(text, stars)\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluating the Model\n",
    "\n",
    "Now that we have a model, we have to test it to see how effective it is. Many metrics exist for evaluating performance, however we will be focusing primarily on the accuracy of the model's prediction against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.07      0.13      1403\n",
      "           2       0.00      0.00      0.00       730\n",
      "           3       0.00      0.00      0.00      1175\n",
      "           4       0.43      0.01      0.01      2179\n",
      "           5       0.46      1.00      0.63      4513\n",
      "\n",
      "    accuracy                           0.46     10000\n",
      "   macro avg       0.37      0.22      0.15     10000\n",
      "weighted avg       0.44      0.46      0.30     10000\n",
      "\n",
      "### TESTING test_model: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def test_model_test(test_model):\n",
    "    test_model()\n",
    "\n",
    "@test\n",
    "def test_model(verbose=True):\n",
    "    # Load model and test data\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    with open(test_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # See the accuracy of our prediction\n",
    "    pred = model.predict(text)\n",
    "    accuracy = metrics.accuracy_score(stars, pred)\n",
    "\n",
    "    if verbose:\n",
    "        print('Accuracy: ', accuracy)\n",
    "    \n",
    "    # Get a little more insight\n",
    "    if verbose:\n",
    "        print(metrics.classification_report(stars, pred))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty bad performance! Let's look into how we can improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Improving the Model\n",
    "\n",
    "There are a few different ways to improve our model. We will go through a couple simple methods, and then discuss a \n",
    "few more in depth techniques for improvement.\n",
    "\n",
    "The first method (and easiest for us to implement) is to change the classification algorithm. Sklearn provides many different machine learning algorithms. Let's try a few out and see which gives us the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB:  0.4619\n",
      "RandomForestClassifier:  0.4513\n",
      "DecisionTreeClassifier:  0.4595\n",
      "SGDClassifier:  0.6336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 0.4619\n",
    "train_model_pipe(MultinomialNB())\n",
    "accuracy = test_model(False)\n",
    "print('MultinomialNB: ', accuracy)\n",
    "\n",
    "# 0.4513\n",
    "train_model_pipe(RandomForestClassifier(n_estimators=10, max_depth=2, random_state=0))\n",
    "accuracy = test_model(False)\n",
    "print('RandomForestClassifier: ', accuracy)\n",
    "\n",
    "# 0.46\n",
    "train_model_pipe(DecisionTreeClassifier())\n",
    "accuracy = test_model(False)\n",
    "print('DecisionTreeClassifier: ', accuracy)\n",
    "\n",
    "# 0.6336\n",
    "train_model_pipe(SGDClassifier())\n",
    "accuracy = test_model(False)\n",
    "print('SGDClassifier: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above, just by changing the model we use we can improve the prediction ability by nearly 20%. Other ways to improve the accuracy of the model include:\n",
    "* __Parameter tuning via grid search:__ Models contain tunable hyperparameters that affect how the model behaves. While it's possible to tune these by hand, you can also do a grid search on those parameters within some range. What this will do is find the most optimal set of parameters within that range, allowing you to get the \"best\" model for that algorithm.\n",
    "* __Different amounts of training data:__ Increasing or decreasing the amount of data used to train the model can affect the end model. Interestingly, more data is not always better, so it's good to try different inputs.\n",
    "* __Different training data:__ In this case, there are over 6 million reviews available to train from. It's possible that a different set of 10,000 reviews than the one we used provides the optimal model.\n",
    "* __Remove (or don't remove) certain words:__ In our example, we removed words we thought shouldn't be trained on. In reality, perhaps including those words or removing others would create a better model. Sklearn includes functionality to see the relative weights per feature of the model, allowing you to see how \"important\" each word in the text is. If the model is weighing words as important that shouldn't be, you can remove those.\n",
    "* __Try different algorithms:__ Sklearn has more algorithms that you could try out, and other machine learning libraries can provide better functionality depending on your use case.\n",
    "* __Change algorithm training parameters:__ Each of the models we used above takes a number of tunable parameters that can affect the end model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Further Reading\n",
    "\n",
    "You should now have a basic understanding on how to clean data, train and optimize a model, save it for later, and then load it to make predictions. This can be utilized in any context that needs a machine learning model.\n",
    "\n",
    "Exercise for the reader: You could aggregate all the reviews for a specific restaurant, and then use the above model on each individual review. By averaging the predicted ratings, you could predict the popularity of a restaurant on Yelp.\n",
    "\n",
    "#### More Information\n",
    "* [Sklearn classification methods](https://stackabuse.com/overview-of-classification-methods-in-python-with-scikit-learn/)\n",
    "* [Tensorflow, another useful ML library](https://www.tensorflow.org/)\n",
    "* [Sklearn model speed optimization](https://scikit-learn.org/stable/developers/performance.html)\n",
    "* [Sklearn hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
