{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Machine Learning Using Scikit-learn\n",
    "\n",
    "In this tutorial, you will learn how to use the scikit-learn (sklearn) library to train a model on a test set of data, and then use this model to make predictions on new data. In addition, you will learn how to verify the correctness and quality of your machine learning model.\n",
    "\n",
    "This tutorial also briefly covers an example in collecting, cleaning, and preparing data for the model, along with how to use Python Pickle to save Python objects to files for later use. The example we work through will show how to save the model, load it up again, and then make predictions on real data, much like you may do in a real project.\n",
    "\n",
    "The model we are going to train will take the text from a yelp review for a restaurant, and predict the rating for that review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up library imports\n",
    "import json\n",
    "import requests\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from testing.testing import test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "The first task in any machine learning application is collecting data. Typically, you will want 2 portions of data: a training set and a test set. The former will be used to train your model, and the latter will be used to test its effectiveness. While you can use 2 entirely different data sets, it is usually sufficient to just break one set into 2 pieces.\n",
    "\n",
    "Because data collection and data parsing are two very different parts in the process, I have chosen to break it up here. Here, we will use the `BeautifulSoup` library to scrape `yelp.com` for all the reviews of a restaurant. While `yelp.com` maintains a useful API, most websites do not. I will collect the data using web scraping because that is more versatile in its application.\n",
    "\n",
    "If you would like to learn more about `BeautifulSoup`, you can visit [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to do code in markdown\n",
    "```python\n",
    "{\n",
    "    'author': 'Aaron W.' # str\n",
    "    'rating': 4.0        # float\n",
    "    'date': '2019-01-03' # str, yyyy-mm-dd\n",
    "    'description': \"Wonderful!\" # str\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING retrieve_html: PASSED 2/2\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def retrieve_html_test(retrieve_html):\n",
    "    status_code, text = retrieve_html(\"http://www.example.com\")\n",
    "    test.equal(status_code, 200)\n",
    "    test.true(\"This domain is established to be used for illustrative examples in documents.\" in text)\n",
    "    # Note that the text hash may change depending on the remote server. Feel free to change the test.\n",
    "\n",
    "@test\n",
    "def retrieve_html(url, params=None, headers=None):\n",
    "    \"\"\"\n",
    "    Return the raw HTML at the specified URL.\n",
    "\n",
    "    Args:\n",
    "        url (string): \n",
    "\n",
    "    Returns:\n",
    "        status_code (integer):\n",
    "        raw_html (string): the raw HTML content of the response, properly encoded according to the HTTP headers.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    return (response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING parse_page: PASSED 4/4\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You do not need to use regular expressions in this solution. This is only for testing.\n",
    "import re\n",
    "\n",
    "def reviews_check(reviews):\n",
    "    type_check = lambda field, typ: all(field in r and typ(r[field]) for r in reviews)\n",
    "    test.true(type_check(\"rating\", lambda r: isinstance(r, float)))\n",
    "    test.true(type_check(\"description\", lambda r: isinstance(r, str)))\n",
    "\n",
    "def parse_page_test(parse_page):\n",
    "    reviews, num_pages = parse_page(retrieve_html(\"https://www.yelp.com/biz/the-porch-at-schenley-pittsburgh\")[1])\n",
    "    reviews_check(reviews)\n",
    "    test.equal(len(reviews), 20)\n",
    "    test.equal(num_pages, 33)\n",
    "\n",
    "# This helper function returns more data than was requested so as to make extract_reviews easier\n",
    "def parse_page_help(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find the reviews\n",
    "    tag = soup.find_all('script', type='application/ld+json')[0]\n",
    "    my_json = json.loads(tag.string)\n",
    "    reviews = my_json['review']\n",
    "    results = []\n",
    "\n",
    "    # Iterate through the reviews json and reformat the data\n",
    "    for cur_review in reviews:\n",
    "        results.append(\n",
    "            {'rating':float(cur_review['reviewRating']['ratingValue']),\n",
    "             'description':cur_review['description']}\n",
    "        )\n",
    "        \n",
    "    # Find the url of the next page\n",
    "    next_page_url = None\n",
    "    temp = soup.find('link', rel='next')\n",
    "    if temp is not None:\n",
    "        next_page_url = temp['href']\n",
    "\n",
    "    # Find how many total pages there are\n",
    "    pages = soup.find_all(string=re.compile('^page', flags=re.IGNORECASE))[0]\n",
    "    index = pages.find('of') + 3\n",
    "    total_pages = int(pages[index:])\n",
    "    \n",
    "    return results, total_pages, next_page_url\n",
    "\n",
    "@test\n",
    "def parse_page(html):\n",
    "    \"\"\"\n",
    "    Parse the reviews on a single page of a restaurant.\n",
    "    \n",
    "    Args:\n",
    "        html (string): String of HTML corresponding to a Yelp restaurant\n",
    "\n",
    "    Returns:\n",
    "        tuple(list, string): a tuple of two elements\n",
    "            first element: list of dictionaries corresponding to the extracted review information\n",
    "            second element: number of pages total\n",
    "    \"\"\"\n",
    "    \n",
    "    results, total_pages, next_page_url = parse_page_help(html)\n",
    "    return results, total_pages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Q 3.5: Extract all of the Yelp reviews for a Single Restaurant\n",
    "\n",
    "So now that we have parsed a single page, and figured out a method to go from one page to the next we are ready to combine these two techniques and actually crawl through web pages! \n",
    "\n",
    "Using `requests`, programmatically retrieve __ALL__ of the reviews for a __single__ restaurant (provided as a parameter). Just like the API was paginated, the HTML paginates its reviews (it would be a very long web page to show 300 reviews on a single page) and to get all the reviews you will need to parse and traverse the HTML. As input your function will receive a URL corresponding to a Yelp restaurant. As output return a list of dictionaries (structured the same as question 3 containing the relevant information from the reviews.\n",
    "\n",
    "Return reviews in the order that they are present on the page.\n",
    "\n",
    "You will need to get the number of pages on the first request and generate the URL for subsequent pages automatically. Use the Yelp website to see how the URL changes for subsequent pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews_test(extract_reviews):\n",
    "    reviews = extract_reviews(\"https://www.yelp.com/biz/larry-and-carols-pizza-pittsburgh\")\n",
    "    test.equal(len(reviews), 46) # This may change!\n",
    "    reviews_check(reviews)\n",
    "\n",
    "@test\n",
    "def extract_reviews(url):\n",
    "    \"\"\"\n",
    "    Retrieve ALL of the reviews for a single restaurant on Yelp.\n",
    "\n",
    "    Parameters:\n",
    "        url (string): Yelp URL corresponding to the restaurant of interest.\n",
    "\n",
    "    Returns:\n",
    "        reviews (list): list of dictionaries containing extracted review information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse the results once so you know how many pages there are\n",
    "    results, total_pages, next_page_url = parse_page_help(retrieve_html(url)[1])\n",
    "\n",
    "    # Iterate total - 1 times, since you already read the first page of reviews\n",
    "    for i in range(total_pages - 1):\n",
    "        new_results, _, temp_url = parse_page_help(retrieve_html(next_page_url)[1])\n",
    "        results = results + new_results\n",
    "        next_page_url = temp_url\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should clean the review description to make it more useful for parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING clean_reviews: PASSED 47/47\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def clean_reviews_test(clean_reviews):\n",
    "    reviews = extract_reviews(\"https://www.yelp.com/biz/larry-and-carols-pizza-pittsburgh\")\n",
    "    og_len = len(reviews)\n",
    "\n",
    "    clean_reviews(reviews)\n",
    "    test.equal(len(reviews), og_len)\n",
    "\n",
    "    for elem in reviews:\n",
    "        desc = elem['description']\n",
    "        test.equal(desc, desc.lower())\n",
    "\n",
    "# Given a description, make it better for model training\n",
    "#      Remove common words\n",
    "#      Make all lowercase\n",
    "#      Remove punctuation\n",
    "def clean_description(review):\n",
    "    desc = review['description']\n",
    "    lower = desc.lower()\n",
    "    \n",
    "    # Remove all characters except a-z and ' '\n",
    "    all_ascii = filter(lambda i: 97 <= ord(i) <= 122 or ord(i) == 32, lower)\n",
    "    word_list = ''.join(all_ascii).split()\n",
    "    \n",
    "    # Remove most of the 100 most common words. Words not removed:\n",
    "    # not, but, out, like, no, into, good, over, well, most\n",
    "    # Taken from https://en.wikipedia.org/wiki/Most_common_words_in_English\n",
    "    common_words = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'his', 'by', 'from', 'them', 'we', 'say', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'or', 'would', 'there', 'their', 'what',\n",
    "        'so', 'up', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
    "        'when', 'make', 'can', 'time', 'just', 'him', 'know', 'take',\n",
    "        'people', 'year', 'your', 'some', 'could', 'them', 'see', 'other',\n",
    "        'than', 'then', 'now', 'look', 'only', 'come', 'its', 'think', 'also',\n",
    "        'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first', 'way',\n",
    "        'even', 'new', 'want', 'because', 'any', 'these', 'give', 'day', 'us'\n",
    "        }\n",
    "    result = [word for word in word_list if word not in common_words]\n",
    "    \n",
    "    review['description'] = ' '.join(result)\n",
    "\n",
    "# Given a list of reviews, clean the descriptions in place\n",
    "@test\n",
    "def clean_reviews(reviews):\n",
    "    list(map(clean_description, reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fe5735ce821e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"halal.pickle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_reviews\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://www.yelp.com/biz/the-halal-guys-new-york-2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-f5f70e336911>\u001b[0m in \u001b[0;36mextract_reviews\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Iterate total - 1 times, since you already read the first page of reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_pages\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnew_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_page_help\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_page_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnew_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnext_page_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-612e58a9a36e>\u001b[0m in \u001b[0;36mparse_page_help\u001b[0;34m(html)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Find the reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'script'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'application/ld+json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mmy_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mreviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"halal.pickle\"\n",
    "with open(filename, 'wb') as file:\n",
    "    reviews = extract_reviews(\"https://www.yelp.com/biz/the-halal-guys-new-york-2\")\n",
    "    print(len(reviews))\n",
    "    pickle.dump(reviews, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "{'rating': 4.0, 'description': \"Not bad.  It's basic pizza, folks.  Just what I wanted.  Can't do the heavy toppings.  This was perfect.\\n3 stars for the food.  1 extra star because I got my order in a half an hour and the price with tip was under 20 bucks.  That's a winner in my book. \\nThey have a big menu to choose from, so I will try them again soon.\"}\n"
     ]
    }
   ],
   "source": [
    "with open(filename, 'rb') as file:\n",
    "    reviews = pickle.load(file)\n",
    "    print(len(reviews))\n",
    "    print(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way to obtain data. We now can start training our model. We will need:\n",
    "    - training data taken from \"https://www.yelp.com/biz/oishii-bento-pittsburgh\"\n",
    "    - testing data taken from \"https://www.yelp.com/biz/mount-everest-sushi-pittsburgh\"\n",
    "   \n",
    "We will:\n",
    "    - train our model on the first set of data using sklearn\n",
    "    - evaluate the accuracy on the testing data\n",
    "    - time how long it takes to make a prediction on average - how good is it?\n",
    "    - save our model to a file\n",
    "    - load a model from a file\n",
    "    - make predictions based on new data\n",
    "    \n",
    "    - Get the rating for a restaurant\n",
    "    - Write a sample program that will use the above to predict the rating of a restaurant based on the reviews\n",
    "\n",
    "Note that this is a supervised regression problem. Explain why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "import sklearn\n",
    "dependent_variable = 'qual_student'\n",
    "x = df[df.columns.difference([dependent_variable])]\n",
    "y = df[dependent_variable]\n",
    "clf = rf(n_estimators = 1000)\n",
    "clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = clf.predict(x)\n",
    "sklearn.metrics.f1_score(y, pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very good! We didn't even cross validate. You'll need to do better :)\n",
    "Let's export this model so we can use it in a microservice (flask api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, '/home/matrix/dockerfile/apps/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_df = pd.DataFrame({ 'age' : pd.Series(1) ,'health' : pd.Series(15) ,'absences' : pd.Series(10)})\n",
    "pred = clf.predict(query_df)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
