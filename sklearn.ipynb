{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Machine Learning Using Scikit-learn\n",
    "#### By Niko Gupta\n",
    "\n",
    "This tutorial will take you through the process, from start to finish, of using machine learning in a real application. You will learn how to use the scikit-learn (sklearn) library to train a model on a test set of data, and then use it to make predictions on new data. In addition, you will learn how to verify the correctness and quality of your machine learning model.\n",
    "\n",
    "This tutorial also covers an example of how to clean and prepare data for the model, along with how to use the Pickle library to save Python objects to files for later use. The example will show how to save the model, load it up again, and then make predictions on real data, much like you might do in a real application.\n",
    "\n",
    "The model we are going to train will take the text from a yelp review for a restaurant, and predict the rating for that review.\n",
    "\n",
    "#### Things the tutorial will cover:\n",
    "* [Data Collection](#Part-1:-Data-Collection)\n",
    "* [Cleaning the Data](#Part-2:-Cleaning-the-Data)\n",
    "* [Training the Model](#Part-3:-Training-the-Model)\n",
    "* [Evaluating the Model](#Part-4:-Evaluating-the-Model)\n",
    "* [Improving the Model](#Part-5:-Improving-the-Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up library imports\n",
    "import json\n",
    "import pickle\n",
    "import requests\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from testing.testing import test\n",
    "\n",
    "# Global variables\n",
    "review_input_file = 'data/review.json'\n",
    "training_data_file = 'training_data.pickle'\n",
    "test_data_file = 'test_data.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Collection\n",
    "\n",
    "The first task in any machine learning application is collecting data. Typically, you will want 2 sets of data: a training set and a test set. The former will be used to train your model, and the latter will be used to test its effectiveness. While you can use 2 entirely different data sets, it is usually sufficient to just break one set into 2 parts.\n",
    "\n",
    "For this tutorial we will be using the [yelp dataset](https://www.yelp.com/dataset/). This dataset was released for students to use in data science applications. It is publicly accessible, but due to Yelp's limitations on on on caching their data, I cannot include the actual dataset in this tutorial. You can download the dataset [here](https://www.yelp.com/dataset/download) to follow along with the tutorial. TODO make preset model\n",
    "\n",
    "The dataset includes interesting information such as business data, reviews, user information (including a pseudo social network through friend mappings), business checkins, restaurant reviews, and photo information. For this tutorial we are interested specifically in the restaurant reviews.\n",
    "\n",
    "One problem we encounter with the yelp dataset is that the uncompressed archive is 8 GB. In particular, the reviews.json file that we are interested in is 5 GB, containing over 6 million revies. If we were to load this entire file as is into RAM, it would cause an OS error. In addition, we don't need all 6 million reviews for training or testing the data. Instead we are going to hardcode the number of reviews we would like per dataset, and only read that many from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_size = 20000\n",
    "\n",
    "# Load the first data_set_size reviews from filename\n",
    "def get_json(filename):\n",
    "    seen = 0\n",
    "    result = []\n",
    "    with open(filename, 'rb') as json_file:\n",
    "        while (seen < data_set_size):\n",
    "            seen += 1\n",
    "            line = json_file.readline()\n",
    "            result.append(json.loads(line))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning the Data\n",
    "\n",
    "Now that we have the reviews file loaded into a dictionary, we need to reformat it into something that is more readily useable by a machine learning model. The operations we are interested in doing to the data in order to clean it are:\n",
    "    - Remove data we don't care about, i.e. removing extra json fields\n",
    "    - Standardize the reviews by removing non alphabetical characters, such as punctuation\n",
    "    - Make all words lowercase so that the model doesn't treat the same words with different case as\n",
    "      different features\n",
    "    - Remove the most common words in the English language (such as 'the' and 'as') so that the model trains on\n",
    "      words that are actually relevant within each review. Certain words that are listed in the top 100 words but\n",
    "      seem like they might be relevant to the review would not make sense to remove. In this case, I chose not to\n",
    "      remove `not`, `but`, `out`, `like`, `no`, `into`, `good`, `over`, `well`, and `most`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING get_datasets: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_datasets_test(get_datasets):\n",
    "    get_datasets()\n",
    "\n",
    "# Given a single review, remove unnecessary columns\n",
    "def remove_extra(review):\n",
    "    review.pop('review_id')\n",
    "    review.pop('user_id')\n",
    "    review.pop('business_id')\n",
    "    review.pop('date')\n",
    "    review.pop('useful')\n",
    "    review.pop('funny')\n",
    "    review.pop('cool')\n",
    "    \n",
    "\n",
    "# Given a single review, clean its description\n",
    "def clean_description(review):\n",
    "    lower = review['text'].lower()\n",
    "    \n",
    "    # Remove all characters except a-z and ' '\n",
    "    all_ascii = filter(lambda i: 97 <= ord(i) <= 122 or ord(i) == 32, lower)\n",
    "    \n",
    "    # TODO: add comment about not removing these words\n",
    "    # Convert the filter object into a list so we can remove common words\n",
    "    word_list = ''.join(all_ascii).split()\n",
    "    common_words = {\n",
    "        'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i',\n",
    "        'it', 'for', 'on', 'with', 'he', 'as', 'you', 'do', 'at',\n",
    "        'this', 'his', 'by', 'from', 'them', 'we', 'say', 'her', 'she',\n",
    "        'or', 'an', 'will', 'my', 'one', 'or', 'would', 'there', 'their', 'what',\n",
    "        'so', 'up', 'if', 'about', 'who', 'get', 'which', 'go', 'me',\n",
    "        'when', 'make', 'can', 'time', 'just', 'him', 'know', 'take',\n",
    "        'people', 'year', 'your', 'some', 'could', 'them', 'see', 'other',\n",
    "        'than', 'then', 'now', 'look', 'only', 'come', 'its', 'think', 'also',\n",
    "        'back', 'after', 'use', 'two', 'how', 'our', 'work', 'first', 'way',\n",
    "        'even', 'new', 'want', 'because', 'any', 'these', 'give', 'day', 'us'\n",
    "        }\n",
    "    result = [word for word in word_list if word not in common_words]\n",
    "    \n",
    "    # Combine the result back into a string, and overwrite the original description\n",
    "    review['text'] = ' '.join(result)\n",
    "\n",
    "# Given a list of reviews, clean the descriptions and remove extra columns.\n",
    "#    Note: this function modifies the input in place\n",
    "def clean_reviews(reviews):\n",
    "    list(map(remove_extra, reviews))\n",
    "    list(map(clean_description, reviews))\n",
    "\n",
    "# Load the data, clean it, and break it into our test and training sets\n",
    "@test\n",
    "def get_datasets():\n",
    "    data = get_json(review_input_file)\n",
    "    clean_reviews(data)\n",
    "\n",
    "    mid = data_set_size // 2\n",
    "    training_data = data[:mid]\n",
    "    test_data = data[mid:]\n",
    "\n",
    "    # Save as pickle objects so we don't have to reload and clean the data every time\n",
    "    with open(training_data_file, 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "    \n",
    "    with open(test_data_file, 'wb') as file:\n",
    "        pickle.dump(test_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Training the Model\n",
    "\n",
    "The first part of training a machine learning model is identifying what kind of problem you have. It can fall into a few different categories:\n",
    "\n",
    "* __Unsupervised learning:__ the model is given a set of inputs, with no target outputs. The model will attempt to find correlations between the inputs, and use that to find correlation with future inputs.\n",
    "\n",
    "* __Supervised learning:__ the model is given both a set of inputs and the target outputs corresponding to those inputs. Depending on the type of the target output, this can be further broken down into classification and regression.\n",
    "\n",
    "    * __Classification:__ output is one of a set of categories. The goal is to find patterns that map the input to the correct category, and then for unstructured input predict which output label to classify it with.\n",
    "\n",
    "    * __Regression:__ output is more continuous. For example, if we were assigning expected GPA for students based on study time, absences, and age, the output would be continuous\n",
    "\n",
    "In our case, the problem is a supervised regression problem. We already know the ratings for each review in the training set, and we want to be able to predict the rating for other future reviews.\n",
    "\n",
    "One other problem comes to light: machine learning models are effectively tuning a complicated equation using the training data, and using that to \"predict\" output on new data. However, we have text data — which isn't easily useable as input for an equation. This means we need to do some further processing on the input datasets before we an use them. One of the most common transformations used in natural language processing is the \"bag of words\" model. A piece of text is parsed into a dictionary mapping each word to the frequency it appears within the text. Then each word is assigned a unique ID, and a separate data structure is kept to maintain this \"vocabulary mapping\". This new mapping of unique ID to frequency is turned into a 2 dimensional matrix, which can be used to train our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = \"model.pickle\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING train_model: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model_test(train_model):\n",
    "    train_model(MultinomialNB())\n",
    "\n",
    "@test\n",
    "def train_model(algorithm):\n",
    "    # Load our training data from the pickle file\n",
    "    with open(training_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # For training, we must separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # Generate our \"bag of words\" frequency mapping\n",
    "    count_vect = CountVectorizer()\n",
    "    bag_of_words = count_vect.fit_transform(text)\n",
    "    #TODO uncomment print(count_vect.vocabulary_) # Visualize the data\n",
    "\n",
    "    # Convert the bag of words to a 2d matrix\n",
    "    transformer = TfidfTransformer()\n",
    "    train_data = transformer.fit_transform(bag_of_words)\n",
    "    \n",
    "    # Training time!\n",
    "    with open(model_file, 'wb') as file:\n",
    "        model = algorithm.fit(train_data, stars)\n",
    "        pickle.dump((model, count_vect, transformer), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to streamline the process of vectorizing and transforming input text, then predicting the output for that input, sklearn provides the `Pipeline` class. This allows us to cleanly package everything together, and the above function simplifies to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### TESTING train_model_pipe: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def train_model_pipe_test(train_model_pipe):\n",
    "    train_model_pipe(MultinomialNB())\n",
    "\n",
    "@test\n",
    "def train_model_pipe(algorithm):\n",
    "    # Load our training data from the pickle file\n",
    "    with open(training_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # For training, we must separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # Note that the names here are arbitrary; they allow us\n",
    "    # to refer back to it later\n",
    "    model = Pipeline([\n",
    "        ('count_vect', CountVectorizer()),\n",
    "        ('transformer', TfidfTransformer()),\n",
    "        ('algorithm', algorithm)\n",
    "    ])\n",
    "    \n",
    "    # Training time!\n",
    "    with open(model_file, 'wb') as file:\n",
    "        model.fit(text, stars)\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluating the Model\n",
    "\n",
    "Now that we have a model, we have to test it to see how effective it is. Many metrics exist for evaluating performance, however we will be focusing primarily on the accuracy of the model against our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4619\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.07      0.13      1403\n",
      "           2       0.00      0.00      0.00       730\n",
      "           3       0.00      0.00      0.00      1175\n",
      "           4       0.43      0.01      0.01      2179\n",
      "           5       0.46      1.00      0.63      4513\n",
      "\n",
      "    accuracy                           0.46     10000\n",
      "   macro avg       0.37      0.22      0.15     10000\n",
      "weighted avg       0.44      0.46      0.30     10000\n",
      "\n",
      "### TESTING test_model: PASSED 0/0\n",
      "###\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niko/Desktop/CMU-Coursework/f19/15388/tutorial/sklearn/my_venv/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def test_model_test(test_model):\n",
    "    test_model()\n",
    "\n",
    "@test\n",
    "def test_model(verbose=True):\n",
    "    # Load model and test data\n",
    "    with open(model_file, 'rb') as file:\n",
    "        model = pickle.load(file)\n",
    "    \n",
    "    with open(test_data_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Separate the input and output\n",
    "    text = list(map(lambda x: x['text'], data))\n",
    "    stars = list(map(lambda x: int(x['stars']), data))\n",
    "    \n",
    "    # See the accuracy of our prediction\n",
    "    accuracy = model.score(text, stars) #TODO what is this\n",
    "    if verbose:\n",
    "        print('Accuracy: ', accuracy)\n",
    "    \n",
    "    # Get a little more insight\n",
    "    if verbose:\n",
    "        pred = model.predict(text)\n",
    "        print(metrics.classification_report(stars, pred))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty bad performance! Let's look into how we can improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Improving the Model\n",
    "\n",
    "There are a few different ways to improve our model. We will go through a couple simple methods, and then discuss a \n",
    "few more in depth techniques for improvement.\n",
    "\n",
    "The first method (and easiest for us to implement) is to change the classification algorithm. Sklearn provides many different machine learning algorithms. Let's try a few out and see which gives us the best model.\n",
    "#cross validation?\n",
    "TODO discuss naive model you used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.50      0.48      1403\n",
      "           2       0.16      0.15      0.15       730\n",
      "           3       0.22      0.19      0.20      1175\n",
      "           4       0.29      0.28      0.29      2179\n",
      "           5       0.63      0.66      0.64      4513\n",
      "\n",
      "    accuracy                           0.46     10000\n",
      "   macro avg       0.35      0.35      0.35     10000\n",
      "weighted avg       0.45      0.46      0.45     10000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#train_model_pipe(MultinomialNB())           # 0.4619\n",
    "#train_model_pipe(SGDClassifier())           # 0.6345\n",
    "#train_model_pipe(SVC(kernel='linear'))      # 0.6391\n",
    "train_model_pipe(DecisionTreeClassifier())      # 0.46\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to improve:\n",
    "    grid?\n",
    "    play with size of training dataset (i.e. data_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Applying the model to a real task\n",
    "\n",
    "TODO: add more info section at the end\n",
    "Do this part if you need extra fluff\n",
    "    - time how long it takes to make a prediction on average - how good is it?\n",
    "\n",
    "    - Get the rating for a restaurant\n",
    "    - Write a sample program that will use the above to predict the rating of a restaurant based on the reviews"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
